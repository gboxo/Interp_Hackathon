### Interpretability of LLM deception


Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training

LARGE LANGUAGE MODELS CAN STRATEGICALLY DECEIVE THEIR USERS WHEN PUT UNDER PRESSURE

Black box access is insufficient for rigorous AI audits


Representation Engineering: A top down approach


AI Deception: A survey of examples

The Internal State of an LLM Knows When Itâ€™s Lying












### AI control : Improving Safety Despite Intentional Subversion


### Debate Helps Supervise Unreliable Experts


### Debating with More Persuasive LLMs Leads to More Truthful Answers

@article{khan2024debating,
  title={Debating with more persuasive llms leads to more truthful answers},
  author={Khan, Akbir and Hughes, John and Valentine, Dan and Ruis, Laura and Sachan, Kshitij and Radhakrishnan, Ansh and Grefenstette, Edward and Bowman, Samuel R and Rockt{\"a}schel, Tim and Perez, Ethan},
  journal={arXiv preprint arXiv:2402.06782},
  year={2024}
}



Self cirtique models to help human evaluators

@article{saunders2022self,
  title={Self-critiquing models for assisting human evaluators},
  author={Saunders, William and Yeh, Catherine and Wu, Jeff and Bills, Steven and Ouyang, Long and Ward, Jonathan and Leike, Jan},
  journal={arXiv preprint arXiv:2206.05802},
  year={2022}
}


Show your work: Scratchpads for intermediate computations

@article{nye2021show,
  title={Show your work: Scratchpads for intermediate computation with language models},
  author={Nye, Maxwell and Andreassen, Anders Johan and Gur-Ari, Guy and Michalewski, Henryk and Austin, Jacob and Bieber, David and Dohan, David and Lewkowycz, Aitor and Bosma, Maarten and Luan, David and others},
  journal={arXiv preprint arXiv:2112.00114},
  year={2021}
}


### Measuring Progress on Scalable Oversight for LLMs


 @article{bowman2022measuring,
 title={Measuring progress on scalable oversight for large language models},
  author={Bowman, Samuel R and Hyun, Jeeyoon and Perez, Ethan and Chen, Edwin and Pettit, Craig and Heiner, Scott and Luko{\v{s}}i{\=u}t{\.e}, Kamil{\.e} and Askell, Amanda and Jones, Andy and Chen, Anna and others},
  journal={arXiv preprint arXiv:2211.03540},
  year={2022}
}


AI Safety via Market Making

@article{hubinger2020,
  title={AI safety via market making},
  author={Evan Hubinger},
  journal={Alignment Forum },
  year={2020}
}








