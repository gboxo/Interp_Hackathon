


@misc{hubinger2024sleeperagentstrainingdeceptive,
      title={Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training}, 
      author={Evan Hubinger and Carson Denison and Jesse Mu and Mike Lambert and Meg Tong and Monte MacDiarmid and Tamera Lanham and Daniel M. Ziegler and Tim Maxwell and Newton Cheng and Adam Jermyn and Amanda Askell and Ansh Radhakrishnan and Cem Anil and David Duvenaud and Deep Ganguli and Fazl Barez and Jack Clark and Kamal Ndousse and Kshitij Sachan and Michael Sellitto and Mrinank Sharma and Nova DasSarma and Roger Grosse and Shauna Kravec and Yuntao Bai and Zachary Witten and Marina Favaro and Jan Brauner and Holden Karnofsky and Paul Christiano and Samuel R. Bowman and Logan Graham and Jared Kaplan and Sören Mindermann and Ryan Greenblatt and Buck Shlegeris and Nicholas Schiefer and Ethan Perez},
      year={2024},
      eprint={2401.05566},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2401.05566}, 
}

@misc{scheurer2024largelanguagemodelsstrategically,
      title={Large Language Models can Strategically Deceive their Users when Put Under Pressure}, 
      author={Jérémy Scheurer and Mikita Balesni and Marius Hobbhahn},
      year={2024},
      eprint={2311.07590},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.07590}, 
}


@inproceedings{Casper_2024, series={FAccT ’24},
   title={Black-Box Access is Insufficient for Rigorous AI Audits},
   url={http://dx.doi.org/10.1145/3630106.3659037},
   DOI={10.1145/3630106.3659037},
   booktitle={The 2024 ACM Conference on Fairness, Accountability, and Transparency},
   publisher={ACM},
   author={Casper, Stephen and Ezell, Carson and Siegmann, Charlotte and Kolt, Noam and Curtis, Taylor Lynn and Bucknall, Benjamin and Haupt, Andreas and Wei, Kevin and Scheurer, Jérémy and Hobbhahn, Marius and Sharkey, Lee and Krishna, Satyapriya and Von Hagen, Marvin and Alberti, Silas and Chan, Alan and Sun, Qinyi and Gerovitch, Michael and Bau, David and Tegmark, Max and Krueger, David and Hadfield-Menell, Dylan},
   year={2024},
   month=jun, pages={2254–2272},
   collection={FAccT ’24} }


@misc{zou2023representationengineeringtopdownapproach,
      title={Representation Engineering: A Top-Down Approach to AI Transparency}, 
      author={Andy Zou and Long Phan and Sarah Chen and James Campbell and Phillip Guo and Richard Ren and Alexander Pan and Xuwang Yin and Mantas Mazeika and Ann-Kathrin Dombrowski and Shashwat Goel and Nathaniel Li and Michael J. Byun and Zifan Wang and Alex Mallen and Steven Basart and Sanmi Koyejo and Dawn Song and Matt Fredrikson and J. Zico Kolter and Dan Hendrycks},
      year={2023},
      eprint={2310.01405},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.01405}, 
}

@misc{park2023aideceptionsurveyexamples,
      title={AI Deception: A Survey of Examples, Risks, and Potential Solutions}, 
      author={Peter S. Park and Simon Goldstein and Aidan O'Gara and Michael Chen and Dan Hendrycks},
      year={2023},
      eprint={2308.14752},
      archivePrefix={arXiv},
      primaryClass={cs.CY},
      url={https://arxiv.org/abs/2308.14752}, 
}


@misc{azaria2023internalstatellmknows,
      title={The Internal State of an LLM Knows When It's Lying}, 
      author={Amos Azaria and Tom Mitchell},
      year={2023},
      eprint={2304.13734},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2304.13734}, 
}

@inproceedings{yang2024interpretability,
  title={INTERPRETABILITY OF LLM DECEPTION: UNIVERSAL MOTIF},
  author={Yang, Wannan and Sun, Chen and Buzsaki, Gyorgy},
  booktitle={Neurips Safe Generative AI Workshop 2024}
}




@article{greenblatt2023ai,
  title={Ai control: Improving safety despite intentional subversion},
  author={Greenblatt, Ryan and Shlegeris, Buck and Sachan, Kshitij and Roger, Fabien},
  journal={arXiv preprint arXiv:2312.06942},
  year={2023}
}



@article{carlsmith2023scheming,
  title={Scheming AIs: Will AIs fake alignment during training in order to get power?},
  author={Carlsmith, Joe},
  journal={arXiv preprint arXiv:2311.08379},
  year={2023}
}





@article{buckshlegeris,
  title={Untrusted smart models and trusted dumb models},
  author={Shlegeris, Buck},
  journal={Alignment Forum },
  year={2023}
}





@article{roger2023preventing,
  title={Preventing Language Models From Hiding Their Reasoning},
  author={Roger, Fabien and Greenblatt, Ryan},
  journal={arXiv preprint arXiv:2310.18512},
  year={2023}
}



@article{perez2022red,
  title={Red teaming language models with language models},
  author={Perez, Ethan and Huang, Saffron and Song, Francis and Cai, Trevor and Ring, Roman and Aslanides, John and Glaese, Amelia and McAleese, Nat and Irving, Geoffrey},
  journal={arXiv preprint arXiv:2202.03286},
  year={2022}
}



@article{zukowski2023responsible,
  title={A Responsible Framework for Super-Alignment: Holistic Perspectives for Human-Machine Interaction are All You Need},
  author={Zukowski, Novak I and Kourouklides, Ioannis and Astolfi, Alessandro},
  year={2023},
  publisher={OSF}
}

@article{michael2023debate,
  title={Debate helps supervise unreliable experts},
  author={Michael, Julian and Mahdi, Salsabila and Rein, David and Petty, Jackson and Dirani, Julien and Padmakumar, Vishakh and Bowman, Samuel R},
  journal={arXiv preprint arXiv:2311.08702},
  year={2023}
}

@article{irving2018ai,
  title={AI safety via debate},
  author={Irving, Geoffrey and Christiano, Paul and Amodei, Dario},
  journal={arXiv preprint arXiv:1805.00899},
  year={2018}
}

@article{perez2022discovering,
  title={Discovering language model behaviors with model-written evaluations},
  author={Perez, Ethan and Ringer, Sam and Luko{\v{s}}i{\=u}t{\.e}, Kamil{\.e} and Nguyen, Karina and Chen, Edwin and Heiner, Scott and Pettit, Craig and Olsson, Catherine and Kundu, Sandipan and Kadavath, Saurav and others},
  journal={arXiv preprint arXiv:2212.09251},
  year={2022}
}

@inproceedings{cotra2021case,
  title={The case for aligning narrowly superhuman models},
  author={Cotra, Ajeya},
  booktitle={AI Alignment Forum},
  pages={5},
  year={2021}
}

@article{khan2024debating,
  title={Debating with more persuasive llms leads to more truthful answers},
  author={Khan, Akbir and Hughes, John and Valentine, Dan and Ruis, Laura and Sachan, Kshitij and Radhakrishnan, Ansh and Grefenstette, Edward and Bowman, Samuel R and Rockt{\"a}schel, Tim and Perez, Ethan},
  journal={arXiv preprint arXiv:2402.06782},
  year={2024}
}




@article{saunders2022self,
  title={Self-critiquing models for assisting human evaluators},
  author={Saunders, William and Yeh, Catherine and Wu, Jeff and Bills, Steven and Ouyang, Long and Ward, Jonathan and Leike, Jan},
  journal={arXiv preprint arXiv:2206.05802},
  year={2022}
}



@article{nye2021show,
  title={Show your work: Scratchpads for intermediate computation with language models},
  author={Nye, Maxwell and Andreassen, Anders Johan and Gur-Ari, Guy and Michalewski, Henryk and Austin, Jacob and Bieber, David and Dohan, David and Lewkowycz, Aitor and Bosma, Maarten and Luan, David and others},
  journal={arXiv preprint arXiv:2112.00114},
  year={2021}
}
 @article{bowman2022measuring,
 title={Measuring progress on scalable oversight for large language models},
  author={Bowman, Samuel R and Hyun, Jeeyoon and Perez, Ethan and Chen, Edwin and Pettit, Craig and Heiner, Scott and Luko{\v{s}}i{\=u}t{\.e}, Kamil{\.e} and Askell, Amanda and Jones, Andy and Chen, Anna and others},
  journal={arXiv preprint arXiv:2211.03540},
  year={2022}
}


@article{hubinger2020,
  title={AI safety via market making},
  author={Evan Hubinger},
  journal={Alignment Forum },
  year={2020}
}




